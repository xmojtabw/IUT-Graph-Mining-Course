| **Domain**                         | **Project / Problem**                                     | **Suggested Approach (method & reference)**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| ---------------------------------- | --------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Social Media / Politics**        | **Rumor detection on social networks (election context)** | Combine text semantics and graph structure. For example, Liu et al. (2025) collected politically-relevant tweets and built a GNN rumor detector: they use a fine-tuned BERT to embed tweet content and construct a **GraphSAGE** network with edge-attention.  This GNN model (SAGEWithEdgeAttention) outperformed traditional methods on election rumors.  We could gather a similar Twitter/PolitiFact dataset, apply BERT for text features, and implement this graph-attention model.                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| **Time-Series Analysis**           | **Time-series classification via graph conversion**       | Convert sequential data to graphs and classify with GNNs.  For example, Coelho *et al.* (2024) propose *Graph Neural Alchemist*, a modular GNN for time-series classification. They convert each time series into a **visibility graph** (nodes = time points, edges by visibility criterion), then apply a GNN.  Their fully-modular architecture (e.g. using directed visibility graphs with in-degree/PageRank features) achieved robust results across tasks.  We could use open time-series datasets (e.g. ECG, sensor data), build visibility graphs, and train a GNN as in this paper.                                                                                                                                     |
| **Molecular Chemistry**            | **Molecular property prediction**                         | Implement a state-of-the-art GNN for molecules.  Li *et al.* (2024) introduced **KA-GNN**, which augments GNN layers with Kolmogorovâ€“Arnold networks and Fourier-series transforms.  The KA-GNN (including KA-GCN and KA-GAT variants) optimizes node embedding, message passing, and readout via these transforms, and it outperformed traditional GNNs on seven molecular benchmarks.  We can implement KA-GNN in PyTorch Geometric and evaluate on datasets like QM9 or MoleculeNet.                                                                                                                                                                                                                                 
| **Neuroscience (Brain Imaging)**   | **Multimodal brain connectivity analysis**                | Integrate multiple brain imaging modalities with GNNs.  For example, Qu *et al.* (2024) fuse fMRI (functional) and DTI (structural) connectivity into one graph framework.  They used a graph model on the Human Connectome Project data to predict cognitive function, adding a masking strategy for connectivity and demonstrating improved predictive accuracy with interpretable key connections.  We might use public brain-connectome datasets (e.g. HCP, UK Biobank) and implement a similar GNN to predict age or cognitive scores, analyzing important network features.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| **Transportation / Traffic**       | **Traffic flow prediction with spatio-temporal GNNs**     | Apply spatio-temporal GNNs to transportation data.  For example, Singh *et al.* (2024) introduce **ISTGCN**, which integrates multiple layers of graph convolutions and temporal convolutions to capture complex traffic dynamics.  On real traffic datasets, ISTGCN achieved significantly lower RMSE/MAE than baselines.  We could train an ST-GNN (e.g. an implementation of ISTGCN or Diffusion GCN) on open traffic data (METR-LA, PEMS) and evaluate short-term flow prediction accuracy.                                                                                                                                                                                                                                   |
| **Knowledge Graphs**               | **Knowledge graph completion (link prediction)**          | Work on KGs by combining GNNs with symbolic methods.  Anil *et al.* (2024) study *inductive* KG completion: learning inference patterns on a training KG and predicting links on a new graph.  They note that pure rule-based methods underperform GNN-based models (e.g. NBFNet).  A project could implement a modern GNN-based link predictor (e.g. R-GCN, NBFNet, or Graph-BERT) on a dataset like FB15k-237 or ogbl-wikikg2, and compare its performance to a rule-mining approach, or implement the hybrid ideas of Anil *et al.*.                                                                                                                                                                                                 |
